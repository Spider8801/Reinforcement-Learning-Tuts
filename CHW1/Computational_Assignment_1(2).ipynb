{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/bolt1299/anaconda3/lib/python3.8/site-packages (from gym) (4.11.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/bolt1299/anaconda3/lib/python3.8/site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/bolt1299/anaconda3/lib/python3.8/site-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/bolt1299/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827622 sha256=e5b5d2222030bb91550340ee69ddb96fdf809bcf443a48d804ddecf885bc22da\n",
      "  Stored in directory: /home/bolt1299/.cache/pip/wheels/17/79/65/7afedc162d858b02708a3b8f7a6dd5b1000dcd5b0f894f7cc1\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, gym\n",
      "Successfully installed gym-0.26.2 gym-notices-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3bMLJMotRAr"
   },
   "source": [
    "Problem 1: Inventory Control\n",
    "===\n",
    "\n",
    "The goal of this problem is to model an inventory control problem with a finite horizon Markov Decision Process (MDP) and derive the optimal solution.\n",
    "\n",
    "Problem Description\n",
    "---\n",
    "Each month, the manager of a warehouse determines current inventory of a single product. Based on this information, he decides whether or not to order additional stock from a supplier. In doing so, he is faced with a tradeoff between the costs associated with ordering and keeping inventory and the lost sales associated with being unable to satisfy a customer demand for the product. The manager's goal is to maximize the profit. Demand for the product is random with a known probability distribution.\n",
    "\n",
    "To see if MDP is a good model for this problem discription, you need to check if the decisions at the current time affects the future. In this case, it does because the current order will determine how many units is available to sell now and also affects how many units will remain for the future.\n",
    "\n",
    "Problem Formulation\n",
    "---\n",
    "To model the problem as a finite horizon MDP, we need to specify a tuple $(S, A, P, r, H)$. Suppose the decision epoch is $H$ months. Let $s_t$ denote the number of units in the warehouse in the beginning of month $t$. Suppose the capacity of the warehouse is $M$. Thus,\n",
    "the state space is $S = \\{0, 1, 2, \\cdots, M\\}$. In state $s$, the manager can order at most $M-s$ units. Thus, the set of admissible controls is $A_{s} = \\{0, 1, \\cdots, M-s\\}$. Suppose the demand $D_t$ at month $t$ has a known time-homogeneous probability distribution $p_j = \\mathbb{P}(D_t=j), j=0, 1, 2, \\cdots$ and the manager orders $a_t$ items. The system dynamics can be represented as $$s_{t+1} = \\max \\{s_t + a_t - D_t, 0\\}.$$\n",
    "\n",
    "The transition probability is then given by\n",
    "\\begin{align}\n",
    "p(j \\mid s, a) =\n",
    "\\begin{cases}\n",
    "0 &j > s + a \\\\\n",
    "p_{s+a-j} & 1\\leq j \\leq s+a \\\\\n",
    "q_{s+a} & j=0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "where $q_u = \\sum_{j=u}^\\infty p_j$ (Why?).\n",
    "\n",
    "It remains to formulate the reward function. We need to account for three different costs/rewards: ordering costs, storage costs, and selling reward. Suppose the ordering cost $O(u)$ consists of a fixed cost and a cost growing with the order size $u$, i.e.,\n",
    "\\begin{align}\n",
    "O(u) =\n",
    "\\begin{cases}\n",
    "K + c(u) & if u > 0 \\\\\n",
    "0 & if u = 0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "Let $h(u)$ be a nondecreasing function denoting the storage cost for $u$ units and suppose there is no cost/reward at the last decision epoch. Finally, if the demand is $j$ units and sufficient units are in the warehouse, a selling reward of $f(j)$ units is obtained. Thus, the reward function can be written as\n",
    "\\begin{align}\n",
    "r_t(s_t, a_t, s_{t+1}) = f(s_t + a_t - s_{t+1}) - h(s_t + a_t) - O(a_t).\n",
    "\\end{align}\n",
    "It is more convenient to work with $r_t(s_t, a_t)$. To this end we compute $F_t(u)$, the expected value of revenue received in month $t$ if there are $u$ units in the warehouse as\n",
    "\\begin{align}\n",
    "F_t(u) = \\sum_{j=0}^{u-1}f(j)p_j + f(u)q_u\n",
    "\\end{align}\n",
    "(Why?). Thus, the reward function is\n",
    "\\begin{align}\n",
    "r_t(s, a) = F(s+a) - h(s+a) - O(a), \\qquad t=1, 2, \\cdots, H-1\n",
    "\\end{align}\n",
    "and $r_H = 0$.\n",
    "\n",
    "Your Job\n",
    "---\n",
    "Let $K = 4, c(u) = 2u, h(u) = u, M=3, H=3, f(u) = 8u$ and\n",
    "\\begin{align}\n",
    "p_j =\n",
    "\\begin{cases}\n",
    "0.25  & j=0 \\\\\n",
    "0.5  & j=1 \\\\\n",
    "0.25  & j=2\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "For your convenience we have hard coded the transition probability and the reward function. Your job is to implement the $\\texttt{optimal_policy_and_value}$ function using the dynamic programming algorithm to compute the optimal policy and value function. Please make sure that your code is in the designated area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "h9XemF3HrgHa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal policy is\n",
      "[[3. 2. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "--------------------\n",
      "The optimal value is\n",
      "[[ 4.1875  2.      0.      0.    ]\n",
      " [ 8.0625  6.25    5.      0.    ]\n",
      " [12.125  10.      6.      0.    ]\n",
      " [14.1875 10.5     5.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "class InventoryControlMDP():\n",
    "    def __init__(self, H=4):\n",
    "        self.H = H\n",
    "        self.S = 4 # M=3 which means there are four states: 0, 1, 2, 3.\n",
    "        self.terminal_reward = np.zeros(self.S)\n",
    "        self.r = self._get_reward()\n",
    "        self.P = self._get_transition()\n",
    "\n",
    "    def _get_reward(self):\n",
    "        r = [[0,-1,-2,-5],[5,0,-3,-np.inf],[6,-1,-np.inf,-np.inf],[5,-np.inf,-np.inf,-np.inf]]\n",
    "\n",
    "        return np.array(r)\n",
    "\n",
    "    def _get_transition(self):\n",
    "\n",
    "        P = np.zeros((self.S,self.S,self.S)) ####p(s,a,s')\n",
    "        p_mat = np.array([[1,0,0,0],[3/4,1/4,0,0],[1/4,1/2,1/4,0],[0,1/4,1/2,1/4]])\n",
    "\n",
    "        for i in range(self.S):\n",
    "            for j in range(self.S-i):\n",
    "                for k in range(self.S):\n",
    "                    P[i][j][k] = p_mat[i+j][k]\n",
    "        return P\n",
    "\n",
    "    def optimal_policy_and_value(self):\n",
    "        \"\"\"\n",
    "        This function should return two numpy arrays denoting\n",
    "        the optimal policy and value function.\n",
    "        \"\"\"\n",
    "        policy = np.zeros((self.S, self.H-1)) # element (s, h) denotes the optimal policy at state s and time h\n",
    "        value = np.zeros((self.S, self.H)) # element (s, h) denotes the value function at state s and time h\n",
    "        ################ Your Code Here ###############\n",
    "\n",
    "        # Initialize the terminal reward for the last decision epoch (H)\n",
    "        value[:, self.H - 1] = self.terminal_reward\n",
    "\n",
    "        # Dynamic programming backward pass\n",
    "        value[:, self.H-1] = self.terminal_reward\n",
    "        for h in range(self.H-2, -1, -1):\n",
    "            expected_future_rewards = np.zeros(self.S)\n",
    "            for s in range(self.S):\n",
    "                # expected_future_rewards = np.zeros(self.S)\n",
    "                for a in range(self.S):\n",
    "                    expected_future_rewards[a] = self.r[s, a] + np.sum(self.P[s, a, :] * value[:, h+1])\n",
    "                total_values = expected_future_rewards\n",
    "                optimal_action = np.argmax(total_values)\n",
    "                policy[s, h] = optimal_action\n",
    "                value[s, h] = total_values[optimal_action]\n",
    "\n",
    "        ################ End of Your Code #############\n",
    "        return policy, value\n",
    "\n",
    "mdp = InventoryControlMDP(H=4)\n",
    "policy, value = mdp.optimal_policy_and_value()\n",
    "print(\"The optimal policy is\")\n",
    "print(policy)\n",
    "print('-'*20)\n",
    "print(\"The optimal value is\")\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6nUhbHmtKaJ"
   },
   "source": [
    "Problem 2: Frozen Lake\n",
    "===\n",
    "The goal of this problem is to get familiar with OpenAI Gym, implement value iteration and policy iteration.\n",
    "\n",
    "Problem Description\n",
    "---\n",
    "OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball. For more information visit https://gym.openai.com.\n",
    "\n",
    "In this computer assigment, you'll get familiar with Frozen Lake environment and implement value and policy iteration algorithms. Frozen Lake is an environment where the agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. For more information please visit https://gym.openai.com/envs/FrozenLake8x8-v0/.\n",
    "\n",
    "Your Job\n",
    "---\n",
    "1. Get started with gym by following the steps here https://gym.openai.com/docs/.\n",
    "2. Read https://gym.openai.com/envs/FrozenLake8x8-v0/ to get familiar with the environment, states, reward function, etc.\n",
    "3. Implement the $\\texttt{value_iteration}$ function below.\n",
    "4. Implement the $\\texttt{policy_iteration}$ function below.\n",
    "5. Answer the questions (By double click on the cell you can edit the cell and put your answer below each question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UZahX5rwpcI5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Gamma=0.9 ----------\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Value iteration took 1.3379015922546387 seconds.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m optimal_v \u001b[38;5;241m=\u001b[39m value_iteration(env, gamma);\n\u001b[1;32m     93\u001b[0m policy \u001b[38;5;241m=\u001b[39m extract_policy(optimal_v, gamma)\n\u001b[0;32m---> 94\u001b[0m policy_score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage score = \u001b[39m\u001b[38;5;124m'\u001b[39m, policy_score)\n",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(env, policy, gamma, n)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_policy\u001b[39m(env, policy, gamma,  n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124;03m\"\"\" Evaluates a policy by running it n times.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    returns:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    average total reward\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     40\u001b[0m             run_episode(env, policy, gamma\u001b[38;5;241m=\u001b[39mgamma, render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(scores)\n",
      "Cell \u001b[0;32mIn[19], line 40\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_policy\u001b[39m(env, policy, gamma,  n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124;03m\"\"\" Evaluates a policy by running it n times.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    returns:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    average total reward\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     scores \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 40\u001b[0m             \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(scores)\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mrun_episode\u001b[0;34m(env, policy, gamma, render)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m     25\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m---> 26\u001b[0m obs, reward, done , _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mint\u001b[39m(\u001b[43mpolicy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     27\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (gamma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step_idx \u001b[38;5;241m*\u001b[39m reward)\n\u001b[1;32m     28\u001b[0m step_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "\n",
    "def run_episode(env, policy, gamma, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma=gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma, epsilon=1e-20, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    This function implements value iteration algorithm for the infinite\n",
    "    horizon discounted MDPs. If the sup norm of v_k - v_{k-1} is less than\n",
    "    epsilon or number of iterations reaches max_iterations, it should return\n",
    "    the value function.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    v = np.zeros(env.observation_space.n)  # initialize value-function\n",
    "    ########################### Your Code Here ###########################\n",
    "    # Hint: see implementation of extract_policy\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.observation_space.n):\n",
    "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(4)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= epsilon):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "\n",
    "\n",
    "    \n",
    "    ########################### End of your code #########################\n",
    "    end = time.time()\n",
    "    print(\"Value iteration took {0} seconds.\".format(end - start))\n",
    "    return v\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v1'\n",
    "    for gamma in [.9, .95, .99, .9999, 1]:\n",
    "        print(\"-\"*10, \"Gamma={0}\".format(gamma) ,\"-\"*10)\n",
    "        env = gym.make(env_name)\n",
    "        #env.seed(1111)\n",
    "        optimal_v = value_iteration(env, gamma);\n",
    "        policy = extract_policy(optimal_v, gamma)\n",
    "        policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "        print('Average score = ', policy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CrV7qJvpcI8"
   },
   "source": [
    "Policy Iteration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2uVHpufypcI8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Gamma=0.9 ----------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FrozenLakeEnv' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGamma=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(gamma) ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     58\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(env_name)\n\u001b[0;32m---> 59\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m(\u001b[38;5;241m1111\u001b[39m)\n\u001b[1;32m     60\u001b[0m optimal_policy \u001b[38;5;241m=\u001b[39m policy_iteration(env, gamma\u001b[38;5;241m=\u001b[39mgamma)\n\u001b[1;32m     61\u001b[0m scores \u001b[38;5;241m=\u001b[39m evaluate_policy(env, optimal_policy, gamma\u001b[38;5;241m=\u001b[39mgamma)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FrozenLakeEnv' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "def compute_policy_v(env, policy, gamma):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s]\n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.observation_space.n):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    This function implements policy iteration algorithm.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    policy = np.random.choice(env.action_space.n, size=(env.observation_space.n))  # initialize a random policy\n",
    "    ########################### Your Code Here ###########################\n",
    "\n",
    "\n",
    "\n",
    "    ########################### End of your code #########################\n",
    "    end = time.time()\n",
    "    print(\"Policy iteration took {0} seconds.\".format(end - start))\n",
    "    return policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v1'\n",
    "    for gamma in [.9, .95, .99, .9999, 1]:\n",
    "        print(\"-\"*10, \"Gamma={0}\".format(gamma) ,\"-\"*10)\n",
    "        env = gym.make(env_name)\n",
    "        #env.seed(1111)\n",
    "        optimal_policy = policy_iteration(env, gamma=gamma)\n",
    "        scores = evaluate_policy(env, optimal_policy, gamma=gamma)\n",
    "        print('Average scores = ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNDLpf2CpcI9"
   },
   "source": [
    "Questions\n",
    "---\n",
    "\n",
    "#### 1. How many iterations did it take for the value iteration to converge? How about policy iteration?\n",
    "Note your answer here.\n",
    "\n",
    "#### 2. How much time did it take for the value iteration to converge? How about the policy iteration?\n",
    "Note your answer here.\n",
    "\n",
    "#### 3. Which algorithm is faster? Why?\n",
    "Note your answer here.\n",
    "\n",
    "#### 4. How does the average score change as $\\gamma$ gets closer to 1? Why?\n",
    "Note your answer here."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
