{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/bolt1299/anaconda3/lib/python3.8/site-packages (0.26.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/bolt1299/anaconda3/lib/python3.8/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/bolt1299/anaconda3/lib/python3.8/site-packages (from gym) (4.11.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/bolt1299/anaconda3/lib/python3.8/site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/bolt1299/anaconda3/lib/python3.8/site-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/bolt1299/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3bMLJMotRAr"
   },
   "source": [
    "Problem 1: Inventory Control\n",
    "===\n",
    "\n",
    "The goal of this problem is to model an inventory control problem with a finite horizon Markov Decision Process (MDP) and derive the optimal solution.\n",
    "\n",
    "Problem Description\n",
    "---\n",
    "Each month, the manager of a warehouse determines current inventory of a single product. Based on this information, he decides whether or not to order additional stock from a supplier. In doing so, he is faced with a tradeoff between the costs associated with ordering and keeping inventory and the lost sales associated with being unable to satisfy a customer demand for the product. The manager's goal is to maximize the profit. Demand for the product is random with a known probability distribution.\n",
    "\n",
    "To see if MDP is a good model for this problem discription, you need to check if the decisions at the current time affects the future. In this case, it does because the current order will determine how many units is available to sell now and also affects how many units will remain for the future.\n",
    "\n",
    "Problem Formulation\n",
    "---\n",
    "To model the problem as a finite horizon MDP, we need to specify a tuple $(S, A, P, r, H)$. Suppose the decision epoch is $H$ months. Let $s_t$ denote the number of units in the warehouse in the beginning of month $t$. Suppose the capacity of the warehouse is $M$. Thus,\n",
    "the state space is $S = \\{0, 1, 2, \\cdots, M\\}$. In state $s$, the manager can order at most $M-s$ units. Thus, the set of admissible controls is $A_{s} = \\{0, 1, \\cdots, M-s\\}$. Suppose the demand $D_t$ at month $t$ has a known time-homogeneous probability distribution $p_j = \\mathbb{P}(D_t=j), j=0, 1, 2, \\cdots$ and the manager orders $a_t$ items. The system dynamics can be represented as $$s_{t+1} = \\max \\{s_t + a_t - D_t, 0\\}.$$\n",
    "\n",
    "The transition probability is then given by\n",
    "\\begin{align}\n",
    "p(j \\mid s, a) =\n",
    "\\begin{cases}\n",
    "0 &j > s + a \\\\\n",
    "p_{s+a-j} & 1\\leq j \\leq s+a \\\\\n",
    "q_{s+a} & j=0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "where $q_u = \\sum_{j=u}^\\infty p_j$ (Why?).\n",
    "\n",
    "It remains to formulate the reward function. We need to account for three different costs/rewards: ordering costs, storage costs, and selling reward. Suppose the ordering cost $O(u)$ consists of a fixed cost and a cost growing with the order size $u$, i.e.,\n",
    "\\begin{align}\n",
    "O(u) =\n",
    "\\begin{cases}\n",
    "K + c(u) & if u > 0 \\\\\n",
    "0 & if u = 0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "Let $h(u)$ be a nondecreasing function denoting the storage cost for $u$ units and suppose there is no cost/reward at the last decision epoch. Finally, if the demand is $j$ units and sufficient units are in the warehouse, a selling reward of $f(j)$ units is obtained. Thus, the reward function can be written as\n",
    "\\begin{align}\n",
    "r_t(s_t, a_t, s_{t+1}) = f(s_t + a_t - s_{t+1}) - h(s_t + a_t) - O(a_t).\n",
    "\\end{align}\n",
    "It is more convenient to work with $r_t(s_t, a_t)$. To this end we compute $F_t(u)$, the expected value of revenue received in month $t$ if there are $u$ units in the warehouse as\n",
    "\\begin{align}\n",
    "F_t(u) = \\sum_{j=0}^{u-1}f(j)p_j + f(u)q_u\n",
    "\\end{align}\n",
    "(Why?). Thus, the reward function is\n",
    "\\begin{align}\n",
    "r_t(s, a) = F(s+a) - h(s+a) - O(a), \\qquad t=1, 2, \\cdots, H-1\n",
    "\\end{align}\n",
    "and $r_H = 0$.\n",
    "\n",
    "Your Job\n",
    "---\n",
    "Let $K = 4, c(u) = 2u, h(u) = u, M=3, H=3, f(u) = 8u$ and\n",
    "\\begin{align}\n",
    "p_j =\n",
    "\\begin{cases}\n",
    "0.25  & j=0 \\\\\n",
    "0.5  & j=1 \\\\\n",
    "0.25  & j=2\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "For your convenience we have hard coded the transition probability and the reward function. Your job is to implement the $\\texttt{optimal_policy_and_value}$ function using the dynamic programming algorithm to compute the optimal policy and value function. Please make sure that your code is in the designated area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h9XemF3HrgHa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal policy is\n",
      "[[3. 2. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "--------------------\n",
      "The optimal value is\n",
      "[[ 4.1875  2.      0.      0.    ]\n",
      " [ 8.0625  6.25    5.      0.    ]\n",
      " [12.125  10.      6.      0.    ]\n",
      " [14.1875 10.5     5.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "class InventoryControlMDP():\n",
    "    def __init__(self, H=4):\n",
    "        self.H = H\n",
    "        self.S = 4 # M=3 which means there are four states: 0, 1, 2, 3.\n",
    "        self.terminal_reward = np.zeros(self.S)\n",
    "        self.r = self._get_reward()\n",
    "        self.P = self._get_transition()\n",
    "\n",
    "    def _get_reward(self):\n",
    "        r = [[0,-1,-2,-5],[5,0,-3,-np.inf],[6,-1,-np.inf,-np.inf],[5,-np.inf,-np.inf,-np.inf]]\n",
    "\n",
    "        return np.array(r)\n",
    "\n",
    "    def _get_transition(self):\n",
    "\n",
    "        P = np.zeros((self.S,self.S,self.S)) ####p(s,a,s')\n",
    "        p_mat = np.array([[1,0,0,0],[3/4,1/4,0,0],[1/4,1/2,1/4,0],[0,1/4,1/2,1/4]])\n",
    "\n",
    "        for i in range(self.S):\n",
    "            for j in range(self.S-i):\n",
    "                for k in range(self.S):\n",
    "                    P[i][j][k] = p_mat[i+j][k]\n",
    "        return P\n",
    "\n",
    "    def optimal_policy_and_value(self):\n",
    "        \"\"\"\n",
    "        This function should return two numpy arrays denoting\n",
    "        the optimal policy and value function.\n",
    "        \"\"\"\n",
    "        policy = np.zeros((self.S, self.H-1)) # element (s, h) denotes the optimal policy at state s and time h\n",
    "        value = np.zeros((self.S, self.H)) # element (s, h) denotes the value function at state s and time h\n",
    "        ################ Your Code Here ###############\n",
    "\n",
    "        value[:, self.H-1] = self.terminal_reward\n",
    "        for h in range(self.H-2, -1, -1):\n",
    "            expected_future_rewards = np.zeros(self.S)\n",
    "            for s in range(self.S):\n",
    "                # expected_future_rewards = np.zeros(self.S)\n",
    "                for a in range(self.S):\n",
    "                    expected_future_rewards[a] = self.r[s, a] + np.sum(self.P[s, a, :] * value[:, h+1])\n",
    "                total_values = expected_future_rewards\n",
    "                optimal_action = np.argmax(total_values)\n",
    "                policy[s, h] = optimal_action\n",
    "                value[s, h] = total_values[optimal_action]\n",
    "\n",
    "\n",
    "\n",
    "        ################ End of Your Code #############\n",
    "        return policy, value\n",
    "\n",
    "mdp = InventoryControlMDP(H=4)\n",
    "policy, value = mdp.optimal_policy_and_value()\n",
    "print(\"The optimal policy is\")\n",
    "print(policy)\n",
    "print('-'*20)\n",
    "print(\"The optimal value is\")\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6nUhbHmtKaJ"
   },
   "source": [
    "Problem 2: Frozen Lake\n",
    "===\n",
    "The goal of this problem is to get familiar with OpenAI Gym, implement value iteration and policy iteration.\n",
    "\n",
    "Problem Description\n",
    "---\n",
    "OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball. For more information visit https://gym.openai.com.\n",
    "\n",
    "In this computer assigment, you'll get familiar with Frozen Lake environment and implement value and policy iteration algorithms. Frozen Lake is an environment where the agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. For more information please visit https://gym.openai.com/envs/FrozenLake8x8-v0/.\n",
    "\n",
    "Your Job\n",
    "---\n",
    "1. Get started with gym by following the steps here https://gym.openai.com/docs/.\n",
    "2. Read https://gym.openai.com/envs/FrozenLake8x8-v0/ to get familiar with the environment, states, reward function, etc.\n",
    "3. Implement the $\\texttt{value_iteration}$ function below.\n",
    "4. Implement the $\\texttt{policy_iteration}$ function below.\n",
    "5. Answer the questions (By double click on the cell you can edit the cell and put your answer below each question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZahX5rwpcI5"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "\n",
    "def run_episode(env, policy, gamma, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma=gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma, epsilon=1e-20, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    This function implements value iteration algorithm for the infinite\n",
    "    horizon discounted MDPs. If the sup norm of v_k - v_{k-1} is less than\n",
    "    epsilon or number of iterations reaches max_iterations, it should return\n",
    "    the value function.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    eps = 1e-10\n",
    "    v = np.zeros(env.observation_space.n)  # initialize value-function\n",
    "    ########################### Your Code Here ###########################\n",
    "    # Hint: see implementation of extract_policy\n",
    "\n",
    "    prev_v = np.copy(v)\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        v[s] = np.max(q_sa)\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "\n",
    "    ########################### End of your code #########################\n",
    "    end = time.time()\n",
    "    print(\"Value iteration took {0} seconds.\".format(end - start))\n",
    "    return v\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v1'\n",
    "    for gamma in [.9, .95, .99, .9999, 1]:\n",
    "        print(\"-\"*10, \"Gamma={0}\".format(gamma) ,\"-\"*10)\n",
    "        env = gym.make(env_name)\n",
    "        #env.seed(1111)\n",
    "        optimal_v = value_iteration(env, gamma);\n",
    "        policy = extract_policy(optimal_v, gamma)\n",
    "        policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "        print('Average score = ', policy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CrV7qJvpcI8"
   },
   "source": [
    "Policy Iteration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uVHpufypcI8"
   },
   "outputs": [],
   "source": [
    "def compute_policy_v(env, policy, gamma):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s]\n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.observation_space.n):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    This function implements policy iteration algorithm.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    policy = np.random.choice(env.action_space.n, size=(env.observation_space.n))  # initialize a random policy\n",
    "    ########################### Your Code Here ###########################\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            end = time.time()\n",
    "            print(\"Policy iteration took {0} seconds.\".format(end - start))\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "\n",
    "\n",
    "\n",
    "    ########################### End of your code #########################\n",
    "    end = time.time()\n",
    "    print(\"Policy iteration took {0} seconds.\".format(end - start))\n",
    "    return policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v1'\n",
    "    for gamma in [.9, .95, .99, .9999, 1]:\n",
    "        print(\"-\"*10, \"Gamma={0}\".format(gamma) ,\"-\"*10)\n",
    "        env = gym.make(env_name)\n",
    "        #env.seed(1111)\n",
    "        optimal_policy = policy_iteration(env, gamma=gamma)\n",
    "        scores = evaluate_policy(env, optimal_policy, gamma=gamma)\n",
    "        print('Average scores = ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNDLpf2CpcI9"
   },
   "source": [
    "Questions\n",
    "---\n",
    "\n",
    "#### 1. How many iterations did it take for the value iteration to converge? How about policy iteration?\n",
    "Value Iteration\n",
    "As we increase the Gamma the number of iteration also increases. It varies from 315 for Gamma=0.9 to 2357 for Gamma=1.\n",
    "Policy Iteration\n",
    "The number of iterations is less as we increrase Gamma. It varies from 8 for Gamma=0.9 to 6 for Gamma=1 I \n",
    "\n",
    "#### 2. How much time did it take for the value iteration to converge? How about the policy iteration?\n",
    "Value Iteration\n",
    "As we inrease Gamma the time for convergence also increases. For Gamma= 0.9 it takes 0.4 seconds and for Gamma=1 it takes 4.7 seconds\n",
    "Policy Iteration\n",
    "The time \n",
    "As we inrease Gamma the time for convergence also increases. For Gamma= 0.9 it takes 0.5 seconds and for Gamma=1 it takes 4.2 seconds\n",
    "#### 3. Which algorithm is faster? Why?\n",
    "The Policy Iteration is a faster algorithm as it requires lesser number of iterations. This is because Value iteration is Computationally heavy\n",
    "\n",
    "#### 4. How does the average score change as $\\gamma$ gets closer to 1? Why?\n",
    "The average score increases as Gamma comes closer to 1. Asymptotically, the closer gamma is to 1, the closer the policy will be to one that optimizes the gains over infinite time. On the other hand, value iteration will be slower to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
