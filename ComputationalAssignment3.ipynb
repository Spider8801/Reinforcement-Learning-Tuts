{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "166c985a",
      "metadata": {
        "id": "166c985a"
      },
      "source": [
        "Cart Pole\n",
        "===\n",
        "In this assignment we implement DQN method for the cart pole problem.\n",
        "\n",
        "Problem Description\n",
        "---\n",
        "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem”. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart. For more information please visit https://www.gymlibrary.dev/environments/classic_control/cart_pole/.\n",
        "\n",
        "Your Job\n",
        "---\n",
        "1. Read https://www.gymlibrary.dev/environments/classic_control/cart_pole/ to understand the environment, states, reward function, etc.\n",
        "2. Implement the DQN method.\n",
        "3. Answer the questions in a pdf.\n",
        "4. Some helpful API documentation can be found here: https://www.gymlibrary.dev/api/core/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00642768",
      "metadata": {
        "id": "00642768"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae8b2e4",
      "metadata": {
        "id": "3ae8b2e4"
      },
      "source": [
        "We use environment 'CartPole-v1'. Run the following cell to test the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63c44cf7",
      "metadata": {
        "id": "63c44cf7"
      },
      "outputs": [],
      "source": [
        "## A demo of operating in this environment with a random policy\n",
        "\n",
        "def run_random_policy(env, gamma, render = False):\n",
        "    \"\"\"\n",
        "    Run 15 episodes by taking random actions.\n",
        "    args:\n",
        "    env: gym environment.\n",
        "    gamma: discount factor.\n",
        "    render: boolean to turn rendering on/off.\n",
        "    \"\"\"\n",
        "    obs = env.reset()\n",
        "    for i in range(15):\n",
        "        step_idx = 0\n",
        "        total_reward = 0\n",
        "        env.reset()\n",
        "        while True:\n",
        "            if render:\n",
        "                env.render()\n",
        "            #choose an action randomly.\n",
        "            #env.step() returns a tuple (state, reward, done) where done is a boolen indicater for episode endings.\n",
        "            obs, reward, done , _ = env.step(env.action_space.sample())\n",
        "            total_reward += (gamma ** step_idx * reward)\n",
        "            step_idx += 1\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "env_name  = 'CartPole-v1'\n",
        "env = gym.make(env_name).unwrapped\n",
        "run_random_policy(env, gamma=0.99, render = True)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff7d6d3f",
      "metadata": {
        "id": "ff7d6d3f"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "     #Input: state\n",
        "     #Output: vector corresponding to state-action values for actions, i.e. Q(s,a).\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        ########################### Your Code Here ###########################\n",
        "\n",
        "        'Define a neural network that takes state as an input and outputs Q(s,a) for each actions.'\n",
        "        'You should use ONLY 1 hidden layer with say 50 hidden nodes and ReLU activation function'\n",
        "\n",
        "        ########################### Your Code Here ###########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ########################### Your Code Here ###########################\n",
        "\n",
        "        'Define the forward method to get output from your neural network for a given state x as input and return this output'\n",
        "\n",
        "        ########################### Your Code Here ###########################\n",
        "\n",
        "\n",
        "def choose_action(model, state, EPSILON = 0.9):\n",
        "    \"\"\"\n",
        "    This is the epsilon-greedy strategy.\n",
        "    \"\"\"\n",
        "\n",
        "    ########################### Your Code Here ###########################\n",
        "\n",
        "    'Write an epsilon greedy strategy to choose action based upon Q(s,a) calculated through your model'\n",
        "    'Return the action'\n",
        "\n",
        "    ########################### Your Code Here ###########################\n",
        "\n",
        "def copy_parameters(local_model, target_model):\n",
        "    \"\"\"\n",
        "    Update model parameters.\n",
        "    local model (PyTorch model): weights will be copied from.\n",
        "    target model (PyTorch model): weights will be copied to.\n",
        "    \"\"\"\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "        target_param.data.copy_(local_param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3472b742",
      "metadata": {
        "id": "3472b742"
      },
      "outputs": [],
      "source": [
        "def DQN_update(model, model_ft, gamma, epochs, render = False):\n",
        "    \"\"\"\n",
        "    Use online learning to update DQN.\n",
        "    args:\n",
        "    model (DQN): DQN model\n",
        "    model_ft (DQN): DQN model for fixed target\n",
        "    \"\"\"\n",
        "    reward_history = []\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) #feel free to tweak the learning rate\n",
        "\n",
        "    #run epoches of episodes\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        state = env.reset()   #reset enviroment. Get initial state.\n",
        "        total_reward = 0\n",
        "\n",
        "        if i%100 == 0: #update the fixed target network parameters every 100 epochs.\n",
        "            copy_parameters(model, model_ft)\n",
        "\n",
        "\n",
        "        ########################### Your Code Here ###########################\n",
        "\n",
        "        'Train the DQN model'\n",
        "        'Use SmoothL1Loss() as the loss function'\n",
        "        'Train it via two methods. '\n",
        "        '  a) Use the original reward returned by the environment for triaining'\n",
        "        '  b) Use the following reward shaping:'\n",
        "            #x, x_dot, theta, theta_dot = state\n",
        "            #r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
        "            #r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
        "            #reward = r1 + r2\n",
        "        'We will check if there is a difference in convergence rate with and without this reward shaping.'\n",
        "\n",
        "        ########################### Your Code Here ###########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f46fbec5",
      "metadata": {
        "id": "f46fbec5"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    gamma = 0.99\n",
        "    np.random.seed(1111)\n",
        "    env_name  = 'CartPole-v1'\n",
        "    env = gym.make(env_name).unwrapped\n",
        "    model = DQN()\n",
        "    model_ft = DQN()\n",
        "    DQN_update(model, model_ft, gamma = 0.99, epochs = 2000, render = True) #Try different epochs until convergence.\n",
        "    env.close()\n",
        "\n",
        "    ## Write any other lines of code if you need it for plotting and other purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07aecd02",
      "metadata": {
        "id": "07aecd02"
      },
      "source": [
        "Questions (Answer in a separate pdf file)\n",
        "---\n",
        "\n",
        "#### 1. Plot the true reward (the reward returned by env.step()) collected against episode during training for both with and without reward shaping case. Note that you still need to plot the true reward in the reward shaping case too.\n",
        "\n",
        "\n",
        "#### 2. Does the performance improve from episode to episode? Did it overfit?\n",
        "\n",
        "\n",
        "#### 3. Why do we need two neural networks? Why do we need the fixed target network (model_ft)?\n",
        "\n",
        "\n",
        "#### 4. How would you modify the above DQN method to double DQN method? Writing a pseudo-code in your answer pdf is fine. You would receive bonus points if you code this method and answer all the other questions for this method as well and provide the code.\n",
        "\n",
        "\n",
        "#### 5. The above code did not use experience replay. What will be the benefit of using experience replay?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}